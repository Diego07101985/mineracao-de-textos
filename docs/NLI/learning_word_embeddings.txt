
Abstract. This paper describes the creation of PT-LKB, new Portuguese word embeddings learned from a large lexical-semantic knowledge base (LKB), using the node2vec method. Resulting embeddings
combine the strengths of word vector representations and, even with
lower dimensions, achieve high scores in genuine similarity, which so far
were obtained by exploiting the graph structure of LKBs.

Keywords: Lexical-semantic knowledge bases
Lexical semantics · Semantic similarity

1

· Word embeddings

Introduction

Distributional word representations of language became a trend in natural language processing, mainly after the development of eﬃcient models that learn
low dimension word vectors (word embeddings) from very large corpora, with
a neural network. Those include word2vec [8] and GloVe [10], extensively used
in semantic similarity and analogy tasks. Distributional models reﬂect how language is used and enable to compute the similarity of words from the cosine
of their vectors. Lexical-semantic networks (LKBs), such as WordNet [2], are
alternative representations of word meaning, more theoretical, and generally
handcrafted or created semi-automatically. A relevant diﬀerence is that, in
LKBs, semantic relations are explicitly labelled, which does not happen in word
embeddings, despite existing work on inducing speciﬁc relations from the word
vectors [15]. Moreover, although there are several algorithms for computing
word similarity [9] and relatedness [1] in LKBs, none is as straightforward as
computing a cosine.
For Portuguese, several LKBs have been developed in the last 15 years and
diﬀerent word embeddings became available in the last two. Some of the previous
were recently compared in word similarity tests [4], where LKBs seemed more
suitable for computing genuine similarity, while word embeddings achieved better
results for relatedness.
c Springer Nature Switzerland AG 2018
A. Villavicencio et al. (Eds.): PROPOR 2018, LNAI 11122, pp. 265–271, 2018.
https://doi.org/10.1007/978-3-319-99722-3_27

266

H. Gon¸calo Oliveira

This work describes how word embeddings can be learned from the structure of a large Portuguese LKB, using node2vec [6], a representation learning
method for networks, which represents node neighbourhoods in a d-dimensional
feature space, by applying a biased random walk procedure. Running node2vec
in a Portuguese LKB results in PT-LKB, a collection of new word embeddings,
where word similarity can be computed by the vector cosine, but where high
scores can also be obtained for genuine similarity. This is conﬁrmed by the
results reported in four word similarity tests and compared to those obtained
with other embeddings. It is also worth noticing that, due to the structure of
the LKBs, new embeddings can use vectors with lower dimensions than the
common 300.

2

Setup

Instead of picking one of the available LKBs for Portuguese, we built on previous
work [4], where a large Portuguese LKB was created with relation instances in
ten open lexical resources. This LKB is available online1 in a single text ﬁle,
with a relation instance in each line, followed by the number of lexical resources
where it was found (e.g., fruto HIPERONIMO DE tomate 3). This number can be
seen as a sign of consensus on the validity/utility of the relation.
To learn the PT-LKB embeddings, a C++ implementation of node2vec, available as an example of the Stanford Network Analysis Platform2 , was used. This
required the conversion of the LKB into a graph format accepted by node2vec:
a ﬁle with a pair of related words in each line, followed by their weight, set to
the number provided in the original LKB (e.g., fruto tomate 3). The relation
name, not used by the embeddings, was lost in this process.
To take further conclusions, diﬀerent embeddings were learned by changing
the parameters of the algorithm, starting with the network used, either with
all the relation instances (All ) or only those in at least two, three, four, or ﬁve
resources (In2-5 ), and either considering the weights or not. Other parameters
include the dimension of the vectors (Dim), the walk length (Len(Walk)) and the
number of walks (#Walks). Embeddings were ﬁrst learned with default node2vec
parameters (128, 80, 10), but other parameters were also tested, namely more
walks (200) with a lower length (30), as well as a lower (64) and higher vector
dimension (300, the most common in corpus-based embeddings).

3

Results in Word Similarity Tests

Resulting word embeddings were used to answer four Portuguese word similarity tests: PT-65 [5] and the Portuguese versions [12] of SimLex-999 (SL999), WordSim-353 (WS-535) and RareWords (RW). These tests contain pairs
1
2

http://ontopt.dei.uc.pt/index.php?sec=download outros.
http://snap.stanford.edu/node2vec/.

Word Embeddings from Portuguese LKBs

267

of Portuguese words and a gold similarity/relatedness score based on human
judgements (e.g., p´assaro grua 0.24 or menino rapaz 3.58).
Validation consisted of measuring the correlation (Pearson ρ) between the
gold scores with those obtained by the cosine of the word vectors in the embeddings. The obtained results were later compared to those of other embeddings
available for Portuguese, namely a selection of the LX-DSemVectors [13] and
of the NILC embeddings [7], all learned from large corpora, and also the Numberbatch embeddings [14] for Portuguese words, which combine data from the
ConceptNet semantic network and other sources, including word2vec and GloVe.
Before computing similarity, the coverage of the words in the tests by the
embeddings was analysed. This is relevant for further experiments where, when
a word in a pair was not covered, similarity was set to 0. Tables 1 and 2 show
the proportion of pairs covered by the PT-LKB embeddings and by the other
embeddings tested, respectively. As expected, coverage by the PT-LKB embeddings is high when all instances are used, and decreases when minimum weight
increases. Therefore, in the next experiment, we decided to use only the embeddings with instances in all, 2 or 3 resources (All, In2, In3 ). The coverage of
the corpus-based embeddings is comparable to those of the All embeddings,
90–100%, except for RW, where every pair has a rare word, and coverage is
80–89%, against 71% of All. Numberbatch embeddings have a lower coverage
than the corpus-based, possibly because ConceptNet is not as developed for
Portuguese than for other languages.
Table 1. Word pairs covered by the PT-LKB embeddings.
Inst. PT-65 SL-999 WS-353 RW
All

100%

98%

95%

71%

In2

95%

95%

88%

55%

In3

78%

85%

72%

39%

In4

45%

66%

44%

27%

In5

25%

44%

18%

17%

Table 2. Word pairs covered by other Portuguese embeddings.
Embedding

PT-65 SL-999 WS-353 RW

LX vanilla

100%

LX p 17

97%

98%

80%

100%

97%

98%

87%

NILC (all models) 100%

98%

93%

89%

Numberbatch

96%

90%

47%

98%

268

H. Gon¸calo Oliveira

Table 3. Results in four word similarity tests, with the default parameters of node2vec
in the All-network, Redun2 and Redun3.
Instances Weights ρ
PT-65 SL-999 WS-353 RW
All

Yes

0.89

0.57

0.43

0.29

All

No

0.87

0.56

0.43

0.28

In2

Yes

0.75

0.49

0.20

0.27

In2

No

0.73

0.47

0.19

0.26

In3

Yes

0.59

0.29

0.02

0.24

In3

No

0.56

0.27

0.00

0.24

Table 3 shows the similarity results for the PT-LKB embeddings, with the
default node2vec parameters, considering and not considering the weights. In
every test, ρ is higher for the All embeddings and, though with small diﬀerences,
this happens when the weights are considered, which suggests that the number
of resources where each relation instance was found is a relevant aspect.
Table 4 has the results obtained for PT-LKB embeddings learned with different parameters in node2vec, and shows that, with the exception of PT-65,
the same results can be obtained with a higher number of walks, though with
smaller 64-sized vectors. To have an idea, while the text ﬁle of the NILC embeddings with 300-sized vectors uses about 2.5G, the All embeddings use about
125 MB, 250 MB and 600 MB, respectively with 64, 128 and 300-sized vectors.
This also beneﬁts from the fact that the latter were learned from a LKB, which
only contains lemmas. This does not have a noticeable impact in the similarity
tests but, when it comes to other tasks where words are inﬂected, lemmatisation
has to be made before retrieving the word vector. A similar issue happens for the
Numberbatch embeddings. However, in this case, even though it is only available
with 300-sized vectors, the Portuguese part only uses 108 MB.
Table 4. Results in four word similarity tests, with diﬀerent parameters of node2vec
in the All-network.
Instances Weights Dim Len(Walk) #Walks ρ
PT-65 SL-999 WS-353 RW
All

Yes

All

Yes

All

Yes

All

Yes

All

Yes

All

Yes

64

80

10

0.82

64

30

200

0.85

128

80

10

0.89

128

30

200

0.86

300

80

10

0.88

300

30

200

0.88

0.57

0.43

0.28

0.58

0.45

0.30

0.57

0.43

0.29

0.58

0.45

0.30

0.56

0.41

0.29

0.55

0.43

0.30

Word Embeddings from Portuguese LKBs

269

Table 5. Results of other word embeddings in four word similarity tests.
Source

Model

Dim ρ
PT-65 SL-999 WS-353 RW

ConceptNet Numberbatch

300

0.81

0.63

0.50

0.31

NILC

fastText skip-gram

300

0.78

0.33

0.41

0.42

NILC

word2vec c-bow

600

0.60

0.25

0.33

0.36

NILC

GloVe

300

0.74

0.30

0.32

0.38

NILC

GloVe

600

0.75

0.30

0.30

0.37

LX

word2vec skip-gram (p 17)

300

0.66

0.33

0.48

0.35

LX

word2vec skip-gram (vanilla) 300

0.57

0.23

0.36

0.27

NILC

fastText c-bow

100

0.69

0.22

0.20

0.29

NILC

fastText skip-gram

100

0.71

0.28

0.28

0.40

NILC

word2vec skip-gram

100

0.52

0.22

0.34

0.34

NILC

word2vec c-bow

100

0.38

0.15

0.24

0.31

NILC

GloVe

100

0.72

0.27

0.31

0.37

NILC

Wang2vec skip-gram

100

0.69

0.30

0.36

0.38

NILC

Wang2vec c-bow

100

0.65

0.34

0.36

0.39

All-LKB

Adj-Cos

N/A 0.86

0.58

0.44

0.38

All-LKB

PR-Cos

N/A 0.87

0.61

0.46

0.23

N/A 0.74

0.47

0.30

0.41

CONTO.PT μ

Table 5 shows a selection of results for the same similarity tests. The ﬁrst part
includes the corpus embeddings used in previous work [4]. Here, we highlight
the good performance of Numberbatch, especially in SL-999 and WS-353. This
conﬁrms that combining knowledge in a semantic network with corpus-based
embeddings provides a good balance between similarity and relatedness, as it
happened for word similarity tests in other languages [14]. The main drawback
is the lower coverage of Numberbatch for Portuguese, which explains its low
storage size, but leads to lower scores in RW.
The second part of the table has results obtained with 100-sized vector NILC
embeddings, which, despite having a dimension between the LKB embeddings
with lower dimensions (64 and 128), perform poorly in all tests but RW. The
results in the last part of the table were not obtained with embeddings, but
by exploiting the same LKB with other algorithms, also described in previous
work [4]: (i) similarity of the adjacencies of each word, i.e. directly-connected
words, computed with the cosine similarity (Adj-Cos); (ii) PageRank vectors [11],
obtained after running PageRank once for each word, creating a word vector
with the resulting word ranks, and ﬁnally computing the cosine similarity of the
target word vectors (PR-Cos); (iii) Memberships (μ) of words in the synsets of
CONTO.PT [3], a fuzzy wordnet extracted from the same LKB.

270

H. Gon¸calo Oliveira

Overall, with the default parameters, the PT-LKB embeddings achieve the
best reported results in PT-65 (0.89 vs 0.87), previously obtained with the PRCos algorithm. Yet, even if the PageRank vectors are pre-computed, PR-Cos
requires larger vectors, with size equals to the number of words in the LKB.
In SL-999, the PT-LKB embeddings perform better than any corpus embedding, but are outperformed by Numberbatch and by PR-Cos. When it comes to
WS-353 and RW, the PT-LKB embeddings perform below the corpus embeddings with 300-sized vectors. Yet, on WS-353, even the PT-LKB embeddings
with 64-sized vectors achieve higher results than those of corpus embeddings
with 100-sized vectors.

4

Conclusion

This paper described how node2vec was used to learn new Portuguese word
embeddings from LKBs, dubbed PT-LKB, then validated with word similarity
tests. New embeddings performed well, in some cases better than corpus-based
embeddings, even when using lower-dimensions.
Yet, this should be seen as a preliminary validation. The PT-LKB embeddings should further be tested in other tasks, such as semantic textual similarity
or analogies. In fact, we suspect that they are not suitable for solving analogies in available datasets, as the latter typically contain named entities (e.g.
counties and their capitals), generally not present in a LKB. Other experiments
that may also be performed include analysing the impact of learning this kind of
embeddings from individual LKBs or using diﬀerent weights for diﬀerent relation
types. But this might also depend on the target task. Even though synonymy
and hypernymy are more relevant for genuine similarity, other relations play an
important role for computing relatedness.
The PT-LKB embeddings with best results in the similarity tests are freely
available from http://ontopt.dei.uc.pt/index.php?sec=download outros.