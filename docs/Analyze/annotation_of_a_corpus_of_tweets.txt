

Abstract. This article describes the process of creation and annotation of a
tweets corpus for Sentiment Analysis at sentence level. The tweets were captured using the #masterchefbr hashtag, in a tool to acquire the public stream of
tweets in real time and then annotated based on the six basic emotions (joy,
surprise, fear, sadness, disgust, anger) commonly used in the literature. The
neutral tag was adopted to annotate sentences where there was no expressed
emotion. At the end of the process, the measure of disagreement between
annotators reached a Kappa value of 0.42. Some experiments with the SVM
algorithm (Support Vector Machine) have been performed with the objective of
submitting the annotated corpus to a classiﬁcation process, to better understand
the Kappa value of the corpus. An accuracy of 52.9% has been obtained in the
classiﬁcation process when using both discordant and concordant text within the
corpus.
Keywords: Annotation

Á Emotion Á Tweets Á Corpus

1 Introduction
The identiﬁcation of emotions has been one of the areas of great interest in natural
language processing and machine learning, in an attempt to automatically identify
opinions expressed by users in texts, mainly the ones posted on websites and social
networks. Sentiment analysis is the denomination for the research area of identiﬁcation
of emotions in textual data.
Sentiment analysis requires resources which allow the computational systems to
process textual data so that their outputs represent the actual user’s sentiment at the
time of posting. One of the main features is the annotated textual corpus as a form of
representation of the user’s discourse.
This article aims to describe the annotation process of a textual corpus for Brazilian
Portuguese, based on the emotions expressed in the text. The motivation to the construction of the corpus was the lack of corpus for the Portuguese language to serve as a
basis for the sentiment analysis research. The speciﬁc goal in the work described here
was to build a textual database for multiclass classiﬁcation of textual data using the six

Annotation of a Corpus of Tweets for Sentiment Analysis

basic emotions (joy, surprise, fear, sadness, disgust and anger) proposed by [1] with
texts extracted from social media.
In order to achieve the proposed goal, a corpus was built containing texts extracted
from the Twitter platform regarding the speciﬁc domain of “MasterChef Professionals
Brazil”. The capture of texts occurred through the Twitter API for capturing public
tweets, using the #masterchefbr hashtag as the ﬁlter for obtaining tweets from the
desired domain.
This paper is organized as follows: Sect. 2 discusses papers that deal with corpus in
sentiment analysis; in Sect. 3, the details of the process of construction of the corpus
used in this work are presented; Sect. 4 describes with the analysis of agreement
between the annotators. Section 5 discusses the experiments and results; Sect. 6 presents with the conclusions and future work perspective.

2 Corpus of Sentiment Analysis
According to [2] the literature is not extensive for papers describing the construction of
corpus in Brazilian Portuguese for sentiment analysis, including the annotation process,
its methodologies, the results obtained in this process, the evaluation measures and the
subjectivity degree between annotators. Although a few papers were identiﬁed in the
literature describing the process of constructing corpus for Portuguese, they consist
basically the analysis of sentiment with annotation according to polarity (positive and
negative).
In [3] the construction of a corpus of tweets regarding news comments was carried
out using 3 annotators relying on manual processes. An annotated corpus was built
with 850 tweets of which 425 were annotated as positive and 425 as negatives.
The work of [4] shows the ReLi (Book Review), comprehending 14 different
books, in a total of 12470 sentences annotated according to the polarity at sentence
level, to identify entities within the texts.
In [2] a corpus constructed by news from globo.com is presented, using 1750 texts
annotated with the six basic emotions proposed by Ekman [1], each of which has 250
texts plus 250 of the neutral class.
In the context of the sentiment analysis, the work described in [5] explores an
approach of sentiment analysis of journalistic texts for Brazilian Portuguese. The
objective is to identify the basic emotions, using a supervised machine learning
approach and the multiclass SVM to perform the task of textual classiﬁcation.
In [6], the author performs the sentiment analysis orientated to aspects of the
Portuguese language, exploring methods based on frequency and in machine learning,
comparing experiments with corpus in Portuguese and English, annotated according to
the polarity.
In the construction of a corpus, parameters such as Kappa Coefﬁcient, which is a
statistical method to evaluate the level of agreement between two or more annotators,
are used to measure the degree of relevance of this corpus to serve as the basis for a
computational model.
In Computational Linguistics, according to [2], his limit can vary from researcher to
researcher. In the work of [7], the author says that a corpus is considered acceptable

when its agreement degree reaches a Kappa Coefﬁcient above 0.67. On the other hand,
according to [8], the agreement value must be higher than 0.8, for an annotation to be
considered good enough to be used. In [9], the authors state that rather than relying
exclusively on the Kappa Coefﬁcient, we must analyze factors such as the description
of the annotation process, its detailing, the number of annotations, which guidelines
were applied for annotation and others, in order to evaluate and accept an annotated
corpus with precision.
Obtaining a level of Kappa agreement is not a trivial task when using an approach
with six categories of emotion, mainly because there is no clear distinction on the
boundaries between some of the emotions. This situation is not exclusive to corpus in
Portuguese, as can be seen in [10] with a Kappa of 0.52 for the six emotions, in [11]
with a Kappa of 0.66 for annotation of polarity and in [12] with Kappa between 0.21
and 5.1 for the corpus annotated with the six emotions.

3 Corpus Building Process
The corpus described in this article refers to the domain of MasterChef Professionals
Brazil, a culinary competition program, composed of 16 participants screened from
September to December 2017 by Rede Bandeirantes.
The choice of MasterChef as the domain for the building of the corpus was due to
its large audience and a great interaction between the public through Twitter, thus
generating a large amount of textual content for analysis.
The main motivation for the construction of the corpus was the need for annotated
texts to be used in an ongoing research that aims to perform multiclass sentiment
analysis, using machine learning classiﬁers for data stream. Due to the shortage of
corpus annotated with the six basic emotions for Portuguese, it was necessary to build
an annotated corpus that met this need.
The capture of the tweets occurred through an API (Application Programming
Interface) provided by Twitter to capture the public stream of tweets in real time, the
Streaming API. Using Streaming API it is possible to capture tweets, as well as
parameters such as language, geographical location, user and creation date. To access
the tool, it is necessary to have an active Twitter account, and install the application.
After that, access codes are generated to work with the API.
Once the application was created, a python script was developed to link the Twitter
account to the application via the generated access codes. From then on, the capture
was performed using a search restriction ﬁlter. For the corpus in question, the ofﬁcial
hashtag of the program, “#masterchefbr’’, was used.
The capture of the tweets was performed on all episodes of the MasterChef Professional edition, always from the beginning of the program until one hour after the end
of the program. This period has been chosen because many emotional comments can be
identiﬁed after the end of the program, when users mainly express their opinion about
the elimination of the day.
At the end of the data capture process, we obtained 14 ﬁles with tweets corresponding to each episode, with an average of 50000 tweets each one. In order to reduce
the dimensionality of the data to be annotated due to resource constraints, such as the

Annotation of a Corpus of Tweets for Sentiment Analysis
limited number of annotators, some measures were adopted. The ﬁrst measure was to
deﬁne that the tweets to be annotated would refer to the ﬁnalists of the program, since it
would be possible to follow the trajectory of the participant throughout the program,
and to have the perception of the feeling expressed by the users regarding the participant from the beginning to the end of the program edition. The second measure was
the use of a ﬁlter, using as the keyword the name of the ﬁnalist participant, to select
candidate tweets for annotation.
The annotation process was performed by 3 volunteer annotators. Each text was
annotated by 2 different annotators and, in case of disagreement, a third annotator
analyzed the text and assigned a label. The ﬁrst step in the annotation process was to
annotate 3000 randomly chosen texts after the ﬁlter application. In this phase, two
volunteer annotators participated in the process and each annotator had 3 months to
complete this stage. At the end of this period only 2550 were annotated twice.
The annotation was basically consisted of reading the tweets to identify the presence of one or more of an emotion, if there were, and indicate the predominant emotion
(joy, surprise, fear, sadness, disgust, anger), assigning an intensity to the predominant
emotion (high, medium or low) and a polarity (positive, negative). In case of no
emotion or polarity in the tweet, the annotators rated this tweet as neutral.
To assist annotators in the annotation process, a ﬁle containing 30 tweets previously
labeled exclusively to serve as an annotation template was provided. Along with this
ﬁle, instructions on how annotation should occur, and a list of emotional words distributed among the six basic emotions were also provided. These instructions aim to
level the annotators’ knowledge, in an attempt to reduce subjectivity in the annotation
task. A round of collective annotation was made so that possible doubts were resolved
before the ofﬁcial rounds began. The Frame 1 shows a few examples of tweets labeled
in the test round. Because they are real tweets, some of them may not obey the standard
rules of the language.
After the two participants performed the annotations, the discordant tweets between
annotators were identiﬁed and analyzed by a third annotator, who resolved the conflict
and deﬁned the correct label for these tweets. The next step was to analyze the
annotated corpus, measuring the agreement between annotators.

A. dos Santos et al.

4 Degree of Agreement Between Annotators
It is necessary to use some measure to evaluate the reliability of the agreement between
the annotators of the corpus, before being submitted to some algorithmic processing. In
order to prove that this corpus is adequate to test and evaluate the output of a computational process.
Several measures can be used to evaluate the reliability of agreement. The correct
choice of method depends on the data, the resources used in the annotation process and
the expected result. Methods such as Pearson’s Correlation Coefﬁcient and Kappa
Coefﬁcient are known to ensure the reliability of a corpus. In this work, the Kappa
Coefﬁcient was used as a measure of the reliability among annotators.
Some experiments were performed after the completion of the corpus annotation.
The ﬁrst experiment consisted of ﬁnding the level of agreement between annotators,
calculating the Kappa Coefﬁcient. The calculated value of 0.42 is below the ones
indicated as acceptable in the literature of corpus linguistics. Table 1 shows the confusion matrix of agreement between annotators.
Table 1. Matrix of confusion of concordance between annotators.

Another experiment aimed to verify differences between two annotations when the
same annotator labels a text in different occasions, showing the amount of annotator
subjectivity in the labeling process. There were 200 tweets annotated twice, without the
annotator knowing which of the tweets were duplicated. Tables 2 and 3 show the
confusion matrices of texts annotated twice by annotators 1 and 2, respectively.
Table 2. Matrix of confusion of texts annotated twice by annotator 1.

Annotation of a Corpus of Tweets for Sentiment Analysis

Table 3. Matrix of confusion of texts annotated twice by annotator 2.

The Kappa Coefﬁcient found for annotator 1 was 0.54 and for annotator 2 was 0.59.
These results show a high level of subjectivity in the annotation process, which contributes to the low Kappa value of a corpus, since there is no standardization in the
process of annotation of emotional corpus with many classes.
After the process of extracting the necessary evaluation measures, the next step was
the normalization and preprocessing of the texts that made up the corpus.
Normalization is the transformation of words that are out of the standard into words
called canonical, that is, in the cultured form of a certain language. The “out of
standard” words can be generated by incorrect punctuation, misspellings, acronyms,
Internet slangs, and so on. Depending on the source of extraction of the texts (Web,
Social Networks, evaluation sites), the author tends to have little concern about using
the cultured standard of the language, requiring a standardization process to correct
these problems.
In order to perform normalization of the corpus, the Enelvo tool proposed by [13]
was used to normalize noisy words in user generated content written in Portuguese,
identifying and normalizing spelling errors, Internet slangs, among others.
For the preprocessing of texts, a script developed in Python was used to eliminate
stop-words, remove duplication of spaces and characters, as well as remove special
characters, links and text accents.

5 Experiment and Results
Experiments were performed with the corpus properly normalized and preprocessed
using the SVM algorithm, for textual classiﬁcation of the six basic emotions.
Support Vector Machine (SVM) is a binary classiﬁer proposed by [14], which aims
to build an optimal hyperplane, so that it can separate different classes of data with a
greater precision margin. These hyperplanes are constructed by small subsets of
training data, also called support vectors. The SVM proved to be quite promising in
textual classiﬁcation [15]. However, several classiﬁcation problems have more than
two classes, and to solve this problem, we can use the strategy of combining classiﬁers
generated in binary subproblems as shown in [16].

300

A. dos Santos et al.

The training and testing of SVM classiﬁer was done using Weka, a tool that
provides a collection of machine learning algorithms for data mining tasks [17], using
the standard parameters of the tool and with cross-validation of 10 folds.
The evaluation measures considered in the experiment were accuracy, precision,
recall and F-measure.
Table 4 shows the results of experiments for the classiﬁcation of 1163 discordant
texts, submitted to the method of identiﬁcation of emotions.
Table 4. Emotion classiﬁcation of discordant texts.

Table 5. Emotion classiﬁcation of concordant texts.

On the experiment presented in Table 5, a set of 1387 text with full concordance
among annotators were submitted to emotions classiﬁcation method, reaching a 61.2%
success rate, against the 36.4% success rate for discordant texts. Therefore, it is
noticeable that the sentiment analysis method trained with a set of texts with full
concordance between annotators shows a superior performance when compared to the
same method trained with a set of discordant texts.
Another experiment was performed, mixing discordant and concordant texts, with
the objective of understanding the method’s behavior when there is data of both sorts
within the corpus. The results are shown in Table 6.

Annotation of a Corpus of Tweets for Sentiment Analysis

301

Table 6. Evaluation measures using discordant and concordant texts within the corpus.

The accuracy, the measure that calculates the percentage of examples that were
correctly classiﬁed by the classiﬁer, was 52.9%. When analyzing the accuracy percentages obtained in Tables 4, 5 and 6, it can be seen that texts with total agreement
have a better performance when compared to the complete set of texts, making it
evident that a higher concordance rate between the annotators has a direct impact in
classiﬁer learning.
Analyzing the accuracy obtained by the classiﬁer in Table 6, we notice a disproportion between emotions, especially in “disgust” and “surprise”, showing a low rate of
accuracy when compared to the other emotions analyzed. One can attribute this result
to the lack of a clear boundary deﬁnition for such emotions, causing an overlap
between their limits and, therefore, leading to a misclassiﬁcation in the cases that would
fall under either disgust or surprise.

6 Conclusions and Future Work
The purpose of this article was to describe the process of building a corpus of annotated
tweets following Ekman’s six basic emotions. The experiments performed show a low
degree of agreement between the annotators.
However, the results help us understand the great mismatch between emotions,
which in many cases is justiﬁed by the lack of clear boundaries between emotions, as
observed for “disgust” and “surprise” described earlier. This fact leads us to believe
that there may be words and/or expressions invoking both emotions at once. Such lack
of clear boundaries reflected in the concordance between the annotators in the analysis
of some tweets.
Even with Kappa Coefﬁcient results below the limits of acceptance proposed by
some authors, the values obtained in this work are reasonable when compared to
articles that present similar works. Thus, the main contribution of this work is the
construction of a corpus annotated at sentence level based on the six emotions of
Ekman, which will be available for other studies of sentiment analysis in Brazilian
Portuguese.

302

A. dos Santos et al.

For future work, the objective is to expand the number of corpus annotators in an
attempt to obtain a higher Kappa Coefﬁcient, and insert of the irony class, since many
of the texts can be considered ironic.