When, Where, Who, What or Why?
A Hybrid Model to Question Answering
Systems
Eduardo G. Cortes(B) , Vinicius Woloszyn , and Dante A. C. Barone
PPGC, Institute of Informatics, Federal University of Rio Grande Do Sul (UFRGS),
Caixa Postal 15.064, Porto Alegre, RS 91.501-970, Brazil
egcortes@inf.ufrgs.br

Abstract. Question Answering Systems is a ﬁeld of Information
Retrieval and Natural Language Processing that automatically answers
questions posed by humans in a natural language. One of the main steps
of these systems is the Question Classiﬁcation, where the system tries to
identify the type of question (i.e. if it is related to a person, time or a
location) facilitate the generation of a precise answer. Machine learning
techniques are commonly employed in tasks where the text is represented
as a vector of features, such as bag–of–words, Term Frequency-Inverse
Document Frequency (TF-IDF) or word embeddings. However, the quality of results produced by supervised algorithms is dependent on the existence of a large, domain-dependent training dataset which sometimes is
unavailable due to labor-intense of manual annotation of datasets. Normally, word embedding presents a related better performance on small
training sets, while bag-of-words and TF-IDF presents better results on
large training sets. In this work, we propose a hybrid model that combines TF-IDF and word embedding in order to provide the answer type
to text questions using small and large training sets. Our experiments
using the Portuguese language, using several diﬀerent sizes of training
sets, showed that the proposed hybrid model statistically outperforms
bag-of-words, TF-IDF, and word embedding approaches.
Keywords: Question answering
Word embedding

1

· Question classiﬁcation

Introduction

Question answering (QA) is a speciﬁc Computer Science task within the ﬁelds
of Information Retrieval and Natural Language Processing that aims to provide
a precise answer to an input question posed by humans in a natural language.
A QA system implementation usually involves diﬀerent areas of Computer Science that vary from advanced natural language processing, information retrieval,
knowledge representation, automated reasoning, to machine learning. Typically
it includes three main components: (i) Question Processing which classify
c Springer Nature Switzerland AG 2018
A. Villavicencio et al. (Eds.): PROPOR 2018, LNAI 11122, pp. 136–146, 2018.
https://doi.org/10.1007/978-3-319-99722-3_14

When, Where, Who, What or Why?

137

the question in diﬀerent classes (e.g. person, time and location) and create a
query for Information Retrieval (IR) system with information from the question
text; (ii) Passage Retrieval which recovers the text passages from a collection of
documents that likely contains the answer to input question; and (iii) Answer
Processing which generates a ﬁnal answer, usually it extracting from the passage
of texts to the words for the answer [2,4,8].
Question classiﬁcation or Answer type recognition is an important stage in
a Question Processing to provide meaningful guidance on the nature of the
response required [20]. The task consists in determining the question class, normally related to the 5WH, the acronyms used in the English language for the
main types of questions (Who?, What?, Where?, When?, Why?, and How?). For
example, the question “Who won the last Nobel Peace Prize?” expects an answer
of type PERSON while the question “When did the man step on the moon?”
expects an answer of type TIME. Once the question class is determined, this
information will be used in the next stages of the QA pipeline [8]. According
to [12], this stage is useful in Passage Retrieval stage to determine the research
strategy to retrieval candidate passages and useful in Answer Processing in order
to select the answers candidates.
Approaches addressed to Answer type recognition commonly rely on rulebased or supervised learning techniques. In rule-based models, hand-written rules
are manually created by an empirical observation of the questions to determine
patterns in the associated text with the class of question type [6,8]. On the
other hand, approaches based on machine learning, such as Convolutional Neural
Network (CNN), Recurrent Neural Network (RNN) and Support Vector Machine
(SVM), have been showing excellent results in question classiﬁcation task [11,13,
19,22]. However, the quality of results produced by these supervised algorithms is
highly dependent on the existence of a large, domain-dependent training dataset.
Normally, word embedding presents a better performance on small training sets,
while bag-of-words and TF-IDF present better results on large training set [15].
In this work, we propose to use a combination between TF-IDF and
Word2Vec on a hybrid model to question classiﬁcation, once we have observed
that the two approaches complement each other. To evaluate our proposed
model, we used a Portuguese dataset [17] and a linear SVM. The results showed
that the hybrid model statistically outperforms the individual models in diﬀerent sizes of tested training sets. The main contributions of this work are listed
below:
– a comparison between Word2Vec, TF–IDF and bag–of–words for question
classiﬁcation task using a Portuguese data set.
– a hybrid model for question classiﬁcation task that statistically outperforms
bag–of–words, TF–IDF and Word2Vec using diﬀerent sizes of training set.
– a end-to-end testing of a QA system on a Portuguese dataset using diﬀerent
strategies for question classiﬁcation.
The rest of this paper is organized as follows. Section 2 discusses related
approaches for question classiﬁcation task. Section 3 presents details of our proposed hybrid approach. Section 4 describes the design of our experiments, and

138

E. G. Cortes et al.

Sect. 5 show and discusses the results. Section 6 summarizes our conclusions and
presents future research directions.

2

Related Works

We found in the literature several works in question classiﬁcation task for QA
systems. Currently, the majority of works are using approaches based on machine
learning. Most of these support the English language but many others are focusing on a non-English language, like Chinese and Spanish.
We have observed that the ﬁrst approaches in question classiﬁcation used
hand-written rules. In [6] was present the QA Topology that employs about
270 hand-written rules to classify a question in approximately 180 categories.
[7] made a hybrid model that uses a rule-based approach to provides semantic
features to a classiﬁer. [1] uses rules based on a sequence of terms with the
possible types in the text to classify a question.
Nowadays, the rule-based approach is no longer widely used due to the significant eﬀort to create a large number of rules and, currently, the use of machine
learning is achieving excellent results to this task [12]. In [27], is proposed a
question classiﬁcation to enhance a QA system that uses an SVM and a question
semantic similarity model to classiﬁcation. The results showed that the approach
has the accuracy of 91.49% better than baseline approaches. A hybrid approach
is proposed in [16] with a model that combines Wh-words, Wh-words position
and question length to increase the accuracy of existing question classiﬁcation
system in Bangla language.
Deep learning has been widely used in question classiﬁcation task with the
models CNN and RNN. [9] reports that a CNN with little hyperparameter tuning
and static vectors produces excellent results on diﬀerent benchmarks. In order to
deal with long-range dependencies in LSTM (Long short-term memory) models,
[26] proposes a novel architecture that connects continuous hidden states from
previous steps to the current step and brings a consideration mechanism to
the hidden states. [21] proposes a novel method to model short texts based on
semantic clustering and CNN. The results show that the strategy achieves the
best performance in datasets TREC and Google snippets. [7] proposed a group of
sparse CNN by embedding a neural version of a dictionary learning to represent
the input question taking into account the answer set. The results showed that
the model outperforms baselines on four datasets.
One important aspect that high inﬂuence on the performance of a machine
learning model is how the input information is represented. [13] propose to consider answer information in question classiﬁcation using a novel group sparse
CNN in which signiﬁcantly outperform strong baselines model in four datasets.
According to [12], the syntactic and semantic features can usually improve the
question classiﬁcation but augment the number of features can introduce noisy
information that can make misclassiﬁcation. Nonetheless, a common drawback
of supervised learning approaches is that the quality of the results is heavily
inﬂuenced by the availability of a large, domain-dependent annotated corpus

When, Where, Who, What or Why?

139

to train the model. Unsupervised and semi-supervised learning techniques, on
the other hand, are attractive because they do not imply the cost of corpus
annotation [18,23–25].

3

Proposed Approach

Regarding machine learning approaches, usually, a document is represented as
a vector of features taken from the text. A typical approach is bag-of-words and
Term Frequency-Inverse Document Frequency (TF-IDF), where each word from
the vocabulary receives a weight value according to the presence in the document.
Another typical approach is related to word embedding models where each word
is represented in a semantic space, for instance, Word2Vec models [14]. Despite
good representation of the semantic space, in our experiments, we observed that
Word2Vec models do not represent well keywords in a question sentence for question classiﬁcation problem since words like “when”, “who” or “how” is not well
represented as bag–of–words and TF-IDF models. Based on that, our hypothesis
is that TF-IDF has a good representation of words that determine the type of
answer of a question, but doesn’t represent well the semantics while Word2Vec
has opposing characteristics. By observing the advantages and disadvantages of
there models, it is possible to presume that they complement each other.
A bag-of-words is a conventional model for representing texts that employ
a vector of features where each feature represents a word in the vocabulary.
Normally, a feature is represented by its word frequency in the text document.
Another way to determine the value of a feature is the TF-IDF, where we set
a weight to the word according to its frequency in the document (TF) and its
inverse frequency in the collection of documents (IDF). The TF-IDF wf,d is
represented by:
N
(1)
wf,d = f reqf,d × log10
nf
where f reqf,d is the word frequency (feature) f in the document d, N is the total
of documents in the dataset and nf is the number of documents that contains
the word f .
An alternative model to overcome the disadvantages of bag–of–words and TFIDF is Word2Vec [14], a word embedding model that allows making semantic
analogies similar to the real world. In order to get advantages from TF-IDF
model and Word2Vec model, our approach consists of a combination of two
models in a single one that contains the semantic representation of words and
TF-IDF representation. A question is a query q represented by a sentence of
words w1 , . . . , wn where each word wi has a vector vi that represents its position
in the semantic space. Therefore, the concatenation between the two models can
be expressed by tf idfq , w2vq,v where:
w2vq,v =

1
n

n

vi
i=1

(2)

140

E. G. Cortes et al.

tf idfq is a vector with s dimensions, where s is the size of words vocabulary,
and each dimension receives the TF-IDF weight of the word wi , in the question
q. w2vq,v is a vector with 300 dimensions, where each dimension receives the
arithmetic average of words in question q for its dimension.

4

Experiment Design

This section describes the dataset used to evaluate the approaches and presents
the models used as baselines. For the tests, we used an open domain dataset in
the Portuguese language. The baseline models selected are approaches commonly
used in question classiﬁcation task presenting good performances. Due to the
low quantity of works in the literature using a dataset in Portuguese with these
features, it was not possible to compare the approaches with other.
4.1

Dataset

Most QA systems in the literature are working with English datasets, on the
other hand, there are few systems working with the Portuguese language. Most
of the system that works with the Portuguese use the collection Chave [17] for
testing and validation. This collection provides question with their answers class,
a collection of documents to consult, and their respective answers. Also, the collection was built by Linguateca for the tasks in CLEF, two recognized institutions, what makes this dataset a standard for QA in the Portuguese language.
The collection was used in CLEF from 2004 to 2008, where the QA systems
needed to provide an answer to an input question looking for this answer in a
raw text in a set of documents made available by the collection.
The Chave collection contains about 4000 questions in Portuguese where
about 1000 provide at least one response. Each question has included a category
and type as well as other information such as identiﬁcation code and year of
creation. This collection is provided by Linguateca, a center of resource for the
computational processing of the Portuguese language where, in order to add
Portuguese on ad-hoc IR task and QA task, the Linguateca has created the Chave
collection. For this research, we selected 2.973 questions and their respective
answers class from the Chave collection. We did not use all the questions because
some did not provide the correct answer class and also some questions were
discarded due to the low number of samples. The distribution of the dataset
used is depicted in Table 1.
The Word2Vec model used in this works was obtained in NILC-Embeddings,
a repository that storage and sharing word embeddings models generated for the
Portuguese language. We used the Word2Vec model with 300 dimensions and
trained with continuous bag-of-words from a large corpus of Brazilian Portuguese
and European Portuguese, from varied sources [5].

When, Where, Who, What or Why?

141

Table 1. Dataset class distribution
Answer type (class) Amount

4.2

DEFINITION

624

LOCATION

545

MEASURE

502

ORGANIZATION

356

PERSON

582

TIME

364

Total

2973

Baselines

A conventional approach for question classiﬁcation is bag-of-words, TF-IDF or
word embedding with an SVM as classiﬁer. Normally, on this task, SVM outperforms other models like Naive Bayes and Decision Tree [27]. Using the collection
Chave for question classiﬁcation, we observed that linear kernel outperforms the
other kernels, following the conclusions of [28], which showed that SVM based
on linear kernel achieve better results than SVM based on polynomial kernel,
RBF kernel or sigmoid kernel. In this way, the model used in this work uses a
linear SVM from scikit-learn library for Python.
To evaluate our proposed hybrid model, we used three commonly and strong
models employed on QA as the baseline with an SVM classiﬁer based on a linear
kernel, as follows: (i) Bag-of-words with word frequency, namely BOW, where
each word in the bag-of-words received its frequency in the question; (ii) TF-IDF,
namely TFIDF, where each vector dimension received the TF-IDF weight of the
word in the question; and (iii) Word2vec, namely W2V, where each sentence is
mapped as the arithmetic average of the vector of question words in semantic
space.

5

Results and Discussions

This section presents the results of tests and discussions. We divided the test into
two steps: the ﬁrst, showed in Fig. 1(a), test the questions classiﬁcation models;
the second, showed in Fig. 1(b), test these models in a complete QA system for
the Portuguese language. In order to get reliable results and robust conclusions
the results presented were generated varying the size of training samples for the
question classiﬁcation model.
In addition to the baselines models, referred as BOW, TFIDF and W2V, we
present here the results from the proposed approach, referred as HYBRID, that
is a combination between TFIDF and W2V.

142

E. G. Cortes et al.

Fig. 1. (a) F1-Score over diﬀerent training set size in question classiﬁcation task. (b)
Correct Answers rate in QA system over diﬀerent question classiﬁcation model and
training set.

5.1

Question Classification Results

In the ﬁrst test, we measure the models with 53 diﬀerent sampling sizes, ranging
from 40 to 2120 the size of the training set. For each sample interaction, was
extracted the corresponding number of samples from the dataset in a random
position. This process is repeated 5 times and is performed through the calculation of the arithmetic average of the results with regard to reducing the noise
by bias in the results and try as many as possible to train and to test all the
dataset. Thus for each interaction, the data that is not used for training will be
used for testing.
The measure used in the graphic in Fig. 1(a) for model evaluation is F1Score, that is the harmonic mean of precision and recall measures. In Table 2 is
presented the ﬁnal precision, recall and F1-Score from results in Fig. 1(a) using
2120 training samples.
The graphic in Fig. 1(a) shows that the HYBRID approach got better results
than the all other models and in all training sample sizes in question classiﬁcation
task. The diﬀerence is 3% points (pp) for F1-Score when compared to the runnerup method, namely TFIDF. Regarding the BOW, the diﬀerence if 3 pp, and W2V
is 8 pp. Using the Wilcoxon statistical test with a signiﬁcance level of 0.05, we
veriﬁed that the results got by HYBRID model are statistically superior to all
baselines models. The Table 2 shows that the HYBRID approach has better
results in recall and precision measures when uses 2120 training samples as well.
When comparing the results of baseline models, BOW and TFIDF had similar results and outperform the model W2V (Word2Vec). With an F1-Score about
5 pp lower than another baseline models in most of the training samples, it is
possible to assume that W2V model alone is not the best option for question
classiﬁcation task. Although the BOW and TFIDF models got the best baseline results, in question classiﬁcation task the amount of information get from
an input question is lower than a complete text, so the W2V model can extract
and complement a model with important semantic information.
While bag–of–words and TF-IDF can handle with important words like
“Who” or “When”, Word2Vec can handle with problems related with semantic

When, Where, Who, What or Why?

143

as synonyms. Thus, based on results, it is possible to assume that the combination of TF-IDF with a Word2Vec model can handle with the disadvantages that
the individual models cannot deal.
Table 2. Recall, Precision and F1-Score with 2120 training samples
Model

Recall

Precision F1-score

BOW

0.8727

0.8717

0.8685

TFIDF

0.8757

0.8752

0.8724

W2V

0.8487

0.8472

HYBRID 0.8964 0.8921

0.8462
0.8923

In Fig. 2 is shown the F1-Score of each class for each question classiﬁcation
model. The classes PERSON and ORGANIZATION had the worst results once
we observed that this type of question has a similar syntactic and semantic
structure. For example, the question “Who won the last championship?” can
expect the name of a people (PERSON class) or a soccer team (ORGANIZATION class). Also, is possible to observe that the HYBRID can join the bests
performances of TFIDF and W2V models once it is the merge between the two
models.

Fig. 2. F1-Score of each class and for each question classiﬁcation model using 2120
samples for training.

5.2

QA System Results

In order to measure the proposed model in a complete QA system, we have
built a simple QA system for Portuguese that used the models tested in the step
of question classiﬁcation. The QA system was built for open domain and data
unstructured, following a default QA architecture [8]. In this task, we expect to
get a graphic with similar results to the ﬁrst test, once the question classiﬁcation
is an important step in QA system pipeline. The results from QA system should
reﬂect the question classiﬁcation model performance. The second test has the

144

E. G. Cortes et al.

same conﬁguration as the ﬁrst one, except that the measure used for evaluation was the accuracy of the answers of the system. This test uses 21 diﬀerent
sampling sizes, ranging from 80 to 1680 the size of the training set.
The QA system developed for this test uses approaches in each pipeline steps
once in this work the main objective is the question classiﬁcation evaluation. The
approaches used in each step of the QA system is described as follow:
– Question Processing: Each question classiﬁcation models exposed in this work
was used for the question classiﬁcation task. The query for IR system was generated considering all words in the question that was not a stopword (irrelevant word).
– Information Retrieval: The Solr search platform was used to index and query
the documents from the collection Chave. For passage retrieval, each phrase
from the retrievals documents that contain at last one entity of the same
type of question class will be selected. For named entity recognition is used
the collection Harem [3] to train a Conditional Random Field model [10] to
identify and classify the entities of each passage.
– Answer Processing: This step retrieves the entities from retrieved passages
with the same class type as the question. Thus, is created a rank of these
entities ranked by the votes and document rank where the passage and entity
were retrieved.
The question classiﬁcation approaches used in the QA system has a good performance, however, the other steps have low performance compared to it. Thus,
the results of this test must have much more noise than the ﬁrst test. Even so,
the graphic in Fig. 1(b) shows that the performances of this test followed a similar behavior than the ﬁrst one, adding more reliability in the results obtained in
the ﬁrst test. Regarding the accuracy, the HYBRID model shows values statically relevant, where the diﬀerences are 4 pp better than the runner-up baseline.
We veriﬁed that the results got by the model are statistically superior using the
Wilcoxon statistical test with a signiﬁcance level of 0.05.

6

Conclusion

In this work, we proposed a hybrid model combining the features from TF-IDF
and Word2Vec to represent texts for question classiﬁcation task, an important
step in a Question Answering system. To evaluate our approach, we used a linear
Support Vector Machine classiﬁer for all tested models, including the proposed
approach. We used the dataset Chave, a Portuguese collection of questions and
their respective answers where each question in the collection contained the type
of response information.
For evaluating, we have varied the size of training samples generating a
graphic with the F1-Score of each model for each sample. The results obtained
in our experiments, testing the models individually and in a full QA system,
showed that the proposed Hybrid approach overcomes the performance of the

When, Where, Who, What or Why?

145

other baseline models evaluated, which indicates that the concatenation approach between TF-IDF and Word2Vec is promising in the question classiﬁcation
task for Question Answering systems.
For future works, we consider evaluating our approach using others dataset
in another language, for instance in English, in order to compare our approach
with other known models with a more used dataset. Also, we intend to use this
approach in Deep Learning models, like LSTM and CNN.

References
1. Amaral, C., et al.: Priberam’s question answering system in QA@CLEF 2007.
In: Peters, C., et al. (eds.) CLEF 2007. LNCS, vol. 5152, pp. 364–371. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-3-540-85760-0 46
2. Cavalin, P., et al.: Building a question-answering corpus using social media and
news articles. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, A. (eds.)
PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 353–358. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-41552-9 36
3. Freitas, C., Mota, C., Santos, D., Oliveira, H.G., Carvalho, P.: Second harem:
advancing the state of the art of named entity recognition in portuguese. In: LREC.
Citeseer (2010)
4. Gon¸calves, P.N., Branco, A.H.: A comparative evaluation of QA systems over list
questions. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, A. (eds.)
PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 115–121. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-41552-9 11
5. Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., Aluisio, S.:
Portuguese word embeddings: evaluating on word analogies and natural language
tasks. arXiv preprint arXiv:1708.06025 (2017)
6. Hovy, E., Hermjakob, U., Ravichandran, D.: A question/answer typology with
surface text patterns. In: Proceedings of the Second International Conference on
Human Language Technology Research, pp. 247–251. Morgan Kaufmann Publishers Inc. (2002)
7. Huang, Z., Thint, M., Qin, Z.: Question classiﬁcation using head words and their
hypernyms. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 927–936. Association for Computational Linguistics
(2008)
8. Jurafsky, D., Martin, J.H.: Speech and Language Processing, vol. 3. Pearson, London (2014)
9. Kim, Y.: Convolutional neural networks for sentence classiﬁcation. arXiv preprint
arXiv:1408.5882 (2014)
10. Laﬀerty, J., McCallum, A., Pereira, F.C.: Conditional random ﬁelds: probabilistic
models for segmenting and labeling sequence data (2001)
11. Lee, J.Y., Dernoncourt, F.: Sequential short-text classiﬁcation with recurrent and
convolutional neural networks. arXiv preprint arXiv:1603.03827 (2016)
12. Loni, B.: A survey of state-of-the-art methods on question classiﬁcation (2011)
13. Ma, M., Huang, L., Xiang, B., Zhou, B.: Group sparse CNNs for question classiﬁcation with answer sets. arXiv preprint arXiv:1710.02717 (2017)
14. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and phrases and their compositionality. In: Advances in Neural
Information Processing Systems, pp. 3111–3119 (2013)

